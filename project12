<!DOCTYPE html>
<html>
<head>
    <title>Credit Risk Predictions – Salomon Alvarez</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="stylesheet" href="assets/css/main.css" />
</head>
<body class="is-preload">
<div id="wrapper">

    <header id="header">
        <div class="inner">
            <a href="index.html" class="logo">
                <span class="symbol"><img src="images/logo.svg" alt="" /></span><span class="title">Salomon Alvarez</span>
            </a>
            <nav>
                <ul><li><a href="#menu">Menu</a></li></ul>
            </nav>
        </div>
    </header>

    <nav id="menu">
        <h2>Menu</h2>
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="portfolio.html">Portfolio</a></li>
            <li><a href="about.html">About Me</a></li>
            <li><a href="blog.html">Blog</a></li>
            <li><a href="contact.html">Contact</a></li>
        </ul>
    </nav>

    <div id="main">
        <div class="inner">
            <header>
                <h1>Credit Risk Predictions</h1>
                <p><strong>Predicting loan default risk using resampling techniques and ensemble learning methods.</strong></p>
            </header>

            <h2>Summary</h2>
            <p>
                Peer-to-peer lending services like LendingClub allow investors to offer loans directly to borrowers. In this project, machine learning techniques were applied to help assess credit risk. Given the highly imbalanced nature of credit datasets—where most loans are good and few are at risk—we used both resampling strategies and ensemble learning methods to improve prediction performance.
            </p>

            <h2>Approach</h2>
            <h3>Resampling</h3>
            <p>
                Using the <code>imbalanced-learn</code> library, we explored several resampling techniques:
            </p>
            <ul>
                <li>Naive Random Oversampling</li>
                <li>SMOTE (Synthetic Minority Over-sampling Technique)</li>
                <li>Cluster Centroids Undersampling</li>
                <li>SMOTEENN (combined over- and under-sampling)</li>
            </ul>
            <p>Each approach included:</p>
            <ul>
                <li>Training a logistic regression model</li>
                <li>Evaluating with balanced accuracy score, confusion matrix, and imbalanced classification report</li>
            </ul>

            <h3>Ensemble Learning</h3>
            <p>
                We then tested two powerful ensemble classifiers:
            </p>
            <ul>
                <li>Balanced Random Forest Classifier</li>
                <li>Easy Ensemble AdaBoost Classifier</li>
            </ul>
            <p>Each model was trained and evaluated using standard metrics. Additionally, the feature importances were extracted from the random forest model.</p>

            <h2>Results</h2>
            <h3>Resampling</h3>
            <ul>
                <li><strong>Best balanced accuracy score:</strong> Naive Random Oversampler (0.7163)</li>
                <li><strong>Best recall score:</strong> Naive Random Oversampler (0.71)</li>
                <li><strong>Best geometric mean score:</strong> Naive Random Oversampler (0.72)</li>
            </ul>

            <h3>Ensemble Learning</h3>
            <ul>
                <li><strong>Best balanced accuracy score:</strong> Easy Ensemble Classifier (0.9424)</li>
                <li><strong>Best recall score:</strong> Easy Ensemble Classifier (0.94)</li>
                <li><strong>Best geometric mean score:</strong> Easy Ensemble Classifier (0.93)</li>
                <li><strong>Top 3 features:</strong>
                    <ol>
                        <li>total_rec_prncp – 0.0918</li>
                        <li>total_pymnt_inv – 0.0641</li>
                        <li>total_pymnt – 0.0576</li>
                    </ol>
                </li>
            </ul>

            <p>
                This project highlights the importance of handling class imbalance and demonstrates how ensemble models can outperform simpler models in predicting credit risk. The Easy Ensemble Classifier yielded the most reliable performance across all metrics.
            </p>

            <p><a href="https://github.com/salomon-alvarez/your_repo_name" target="_blank">View the GitHub repository</a></p>
        </div>
    </div>

</div>

<script src="assets/js/jquery.min.js"></script>
<script src="assets/js/browser.min.js"></script>
<script src="assets/js/breakpoints.min.js"></script>
<script src="assets/js/util.js"></script>
<script src="assets/js/main.js"></script>

</body>
</html>
